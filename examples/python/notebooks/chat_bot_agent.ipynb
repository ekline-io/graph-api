{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-community langchain-anthropic tavily-python pandas pygithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GITHUB_APP_ID\")\n",
    "_set_env(\"GITHUB_APP_PRIVATE_KEY\")\n",
    "_set_env(\"GITHUB_REPOSITORY\")\n",
    "\n",
    "# Recommended\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ekline-langgraph-api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import jwt\n",
    "\n",
    "# Function to load the private key from a PEM file\n",
    "def load_private_key(pem_file_path):\n",
    "    with open(pem_file_path, 'r') as pem_file:\n",
    "        private_key = pem_file.read()\n",
    "    return private_key\n",
    "\n",
    "def generate_jwt(app_id, private_key_path):\n",
    "\tprivate_key = load_private_key(private_key_path)\n",
    "\tpayload = {\n",
    "\t\t'iat': int(time.time()),\n",
    "\t\t'exp': int(time.time()) + (10 * 60),  # JWT expiration time (10 minutes)\n",
    "\t\t'iss': app_id\n",
    "\t}\n",
    "\tencoded_jwt = jwt.encode(payload, private_key, algorithm='RS256')\n",
    "\treturn encoded_jwt\n",
    "\n",
    "def get_installation_access_token(app_id, private_key_path, installation_id):\n",
    "\tjwt_token = generate_jwt(app_id, private_key_path)\n",
    "\theaders = {\n",
    "\t\t'Authorization': f'Bearer {jwt_token}',\n",
    "\t\t'Accept': 'application/vnd.github.v3+json'\n",
    "\t}\n",
    "\turl = f'https://api.github.com/app/installations/{installation_id}/access_tokens'\n",
    "\tresponse = requests.post(url, headers=headers)\n",
    "\tresponse_data = response.json()\n",
    "\treturn response_data['token']\n",
    "\n",
    "def list_repo_contents(owner, repo, path, access_token):\n",
    "\theaders = {\n",
    "\t\t'Authorization': f'token {access_token}',\n",
    "\t\t'Accept': 'application/vnd.github.v3+json'\n",
    "\t}\n",
    "\turl = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}'\n",
    "\tresponse = requests.get(url, headers=headers)\n",
    "\treturn response.json()\n",
    "\n",
    "def list_all_files(owner, repo, path='', access_token=None, files=[]):\n",
    "\tcontents = list_repo_contents(owner, repo, path, access_token)\n",
    "\tfor item in contents:\n",
    "\t\tif item['type'] == 'file':\n",
    "\t\t\tfiles.append(item['path'])\n",
    "\t\telif item['type'] == 'dir':\n",
    "\t\t\tlist_all_files(owner, repo, item['path'], access_token, files)\n",
    "\n",
    "def code_search_github_repo(query, access_token):\n",
    "\theaders = {\n",
    "\t\t'Authorization': f'token {access_token}',\n",
    "\t\t'Accept': 'application/vnd.github.v3+json'\n",
    "\t}\n",
    "\turl = f'https://api.github.com/search/code?q={query}&org:ekline-io&type=Code&path:*.md'\n",
    "\tresponse = requests.get(url, headers=headers)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing import Annotated\n",
    "import re\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.agent_toolkits.github.toolkit import GitHubToolkit\n",
    "from langchain_community.utilities.github import GitHubAPIWrapper\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from typing import Optional\n",
    "import requests\n",
    "import time\n",
    "import jwt\n",
    "import os\n",
    "\n",
    "app_id = '903588'\n",
    "installation_id = '51026556'\n",
    "private_key_path = os.environ.get(\"GITHUB_APP_PRIVATE_KEY\")\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n",
    "\n",
    "access_token = get_installation_access_token(app_id, private_key_path, installation_id)\n",
    "\n",
    "@tool\n",
    "def github_fetch_files_from_repo(repo: Optional[str] = None) -> list[str]:\n",
    "\t\"\"\"\n",
    "\tList all the files in a given repo\n",
    "\tArgs:\n",
    "      repo (Optional[str]): repository name\n",
    "\tReturns:\n",
    "      List[str]: List of files from the repository\n",
    "\t\"\"\"\n",
    "\towner = 'ekline-io'\n",
    "\trepo = 'documentation'\n",
    "\tpath = ''  # Root directory or specify a subdirectory\n",
    "\tfiles = []\n",
    "\tlist_all_files(owner, repo, path, access_token, files)\n",
    "\treturn files\n",
    "\n",
    "@tool\n",
    "def search_github_files(query: str) -> list[str]:\n",
    "\t\"\"\"\n",
    "\tSearch for user query and return all the files from a repository which contains string in the query\n",
    "\tArgs:\n",
    "\t\tquery (str): search query\n",
    "\tReturns: \n",
    "\t\tList[str]: List of files from the repository\n",
    "\t\"\"\"\n",
    "\treturn code_search_github_repo(query, access_token)\n",
    "\n",
    "searchTool = TavilySearchResults(max_results=1)\n",
    "class State(TypedDict):\n",
    "\tmessages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "class Assistant:\n",
    "\tdef __init__(self, runnable: Runnable):\n",
    "\t\tself.runnable = runnable\n",
    "\n",
    "\tdef __call__(self, state: State, config: RunnableConfig):\n",
    "\t\twhile True:\n",
    "\t\t\tissues = config.get(\"issues\", None)\n",
    "\t\t\tstate = {**state, \"issues\": issues}\n",
    "\t\t\tresult = self.runnable.invoke(state)\n",
    "\t\t\t# If the LLM happens to return an empty response, we will re-prompt it\n",
    "\t\t\t# for an actual response.\n",
    "\t\t\tif not result.tool_calls and (\n",
    "\t\t\t\tnot result.content\n",
    "\t\t\t\tor isinstance(result.content, list)\n",
    "\t\t\t\tand not result.content[0].get(\"text\")\n",
    "\t\t\t):\n",
    "\t\t\t\tmessages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "\t\t\t\tstate = {**state, \"messages\": messages}\n",
    "\t\t\telse:\n",
    "\t\t\t\tbreak\n",
    "\t\treturn {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You could swap LLMs, though you will likely want to update the prompts when\n",
    "# doing so!\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t\t(\n",
    "\t\t\t\t\"system\",\n",
    "\t\t\t\t\"You are a helpful support assistant for ekline, the company, products. \"\n",
    "\t\t\t\t\" Use the provided github tools that can search for issues, comments on issues, solve issues, create PRs in a repositories to assist the user's queries. \"\n",
    "\t\t\t\t\" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "\t\t\t\t\" If a search comes up empty, expand your search before giving up.\"\n",
    "\t\t\t),\n",
    "\t\t\t(\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "part_1_tools = [searchTool , github_fetch_files_from_repo]\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)\n",
    "\n",
    "builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.set_entry_point(\"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_1_graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_1_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"List all the files in the repo\",\n",
    "    \"List all the files in the repo that contains keyword 'YOUR_EKLINE_TOKEN'\",\n",
    "]\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "# print(part_1_graph.get_state(config=config))\n",
    "\n",
    "_printed = set()\n",
    "for question in tutorial_questions:\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
